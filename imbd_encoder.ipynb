{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout, Dense, Dropout, Input, GlobalAveragePooling1D\n",
    "import tensorflow.keras as keras\n",
    "from util import DynamicPadding\n",
    "from encoder import Encoder\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load Data\n",
    "\n",
    "We take the IMBD data, and sort the training data according to the length of the sequence. Sorting introduces more uniform batch sizes w.r.t. the sequence length which reduces training time considerably if combined with dynamical padding. Furthermore, we crop sequences beyond 200 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "25000 Training sequences\n25000 Validation sequences\n"
    }
   ],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "\n",
    "# sort training data w.r.t. the sequence length\n",
    "seq_length = [len(x) for x in x_train]\n",
    "permuted_indicies = np.argsort(seq_length)\n",
    "x_train, y_train = x_train[permuted_indicies], y_train[permuted_indicies]\n",
    "\n",
    "# crop sequences\n",
    "x_train = [x[:maxlen] for x in x_train]\n",
    "x_val = [x[:maxlen] for x in x_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical Padding\n",
    "\n",
    "We overwrite the Keras Sequence class to support dynamical padding which pads batches only and therefore reduce sequence length. This speeds up training because Transformers training time growths quadratically with the sequence legth. See also [MichaÃ«l Benesty](https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e) contribution for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the data into the Dynamic Padding batch loader\n",
    "train = DynamicPadding(x_train, y_train, batch_size=64)\n",
    "test = DynamicPadding(x_val, y_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Build a Classifier by using a single encoding layer. The architecture is adopted from the official [Keras example](https://keras.io/examples/nlp/text_classification_with_transformer/) by Apoorv Nandan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "encoder_embedding = Encoder(vocab_size + 1, maxlen, embed_dim, num_heads, ffn_units=ff_dim, encoders=1)\n",
    "x = encoder_embedding(inputs)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Encoder has arguments in `__init__` and therefore must override `get_config`.\nEpoch 1/2\n391/391 [==============================] - ETA: 0s - loss: 0.3942 - auc_1: 0.9032 - precision_1: 0.8177 - recall_1: 0.8202WARNING:tensorflow:From C:\\Users\\olive\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: ./imbd_model/prst_model_1\\assets\n391/391 [==============================] - 49s 126ms/step - loss: 0.3942 - auc_1: 0.9032 - precision_1: 0.8177 - recall_1: 0.8202 - val_loss: 0.3273 - val_auc_1: 0.9389 - val_precision_1: 0.8910 - val_recall_1: 0.8182\nEpoch 2/2\n391/391 [==============================] - ETA: 0s - loss: 0.2225 - auc_1: 0.9690 - precision_1: 0.9169 - recall_1: 0.9143INFO:tensorflow:Assets written to: ./imbd_model/prst_model_2\\assets\n391/391 [==============================] - 52s 134ms/step - loss: 0.2225 - auc_1: 0.9690 - precision_1: 0.9169 - recall_1: 0.9143 - val_loss: 0.3567 - val_auc_1: 0.9322 - val_precision_1: 0.8313 - val_recall_1: 0.8920\n"
    }
   ],
   "source": [
    "adam_opt = Adam(0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# save the model after each epoch\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./imbd_model/prst_model_{epoch}',\n",
    "        save_freq='epoch',\n",
    "        ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='./imbd_logs',\n",
    "        profile_batch=0, \n",
    "        )\n",
    "]\n",
    "\n",
    "# define relevant metrics\n",
    "metrics = [\n",
    "    tf.keras.metrics.AUC(),\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall()\n",
    "]\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    optimizer=adam_opt,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=metrics,\n",
    "#     run_eagerly=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train, validation_data=test, callbacks=callbacks, epochs=2, verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}