{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from presentation import Presentation\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout, Dense, Dropout, Input, GlobalAveragePooling1D\n",
    "import tensorflow.keras as keras\n",
    "import util\n",
    "import data_pipeline\n",
    "import pandas as pd\n",
    "from encoder import Encoder\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We take the IMBD data, and sort the training data according to the length of the sequence. Sorting introduces more uniform batch sizes w.r.t. the sequence length which reduces training time considerably if combined with dynamical padding. Furthermore, we crop sequences beyond 200 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "\n",
    "# sort training data w.r.t. the sequence length\n",
    "seq_length = [len(x) for x in x_train]\n",
    "permuted_indicies = np.argsort(seq_length)\n",
    "x_train, y_train = x_train[permuted_indicies], y_train[permuted_indicies]\n",
    "\n",
    "# crop sequences\n",
    "x_train = [x[:maxlen] for x in x_train]\n",
    "x_val = [x[:maxlen] for x in x_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical Padding\n",
    "\n",
    "We overwrite the Keras Sequence class to support dynamical padding which pads batches only and therefore reduce sequence length. This speeds up training because Transformers training time growths quadratically with the sequence legth. See also [MichaÃ«l Benesty](https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e) contribution for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPadding(Sequence):\n",
    "    '''\n",
    "    implements batch sampling with dynamic padding. Credits to\n",
    "\n",
    "    https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n",
    "    '''\n",
    "\n",
    "    def __init__(self, tokenized_seqs, targets, batch_size):\n",
    "        self.y = tokenized_seqs\n",
    "        self.z = targets\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.y) / self.batch_size)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "              self.batch_size]\n",
    "        batch_z = self.z[idx * self.batch_size:(idx + 1) *\n",
    "              self.batch_size]\n",
    "        # padding w.r.t. the longest sequence in the batch\n",
    "        batch_y = pad_sequences(batch_y, padding='post')\n",
    "        return batch_y, batch_z\n",
    "    \n",
    "# dump the data into the Dynamic Padding batch loader\n",
    "train = DynamicPadding(x_train, y_train, batch_size=64)\n",
    "test = DynamicPadding(x_val, y_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Build a Classifier by using a single encoding layer. The architecture is adopted from the official [Keras example](https://keras.io/examples/nlp/text_classification_with_transformer/) by Apoorv Nandan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "encoder_embedding = Encoder(vocab_size + 1, maxlen, embed_dim, num_heads, ffn_units=ff_dim, encoders=1)\n",
    "x = encoder_embedding(inputs)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer Encoder has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/2\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4275 - auc_1: 0.8834 - precision_1: 0.7869 - recall_1: 0.8054INFO:tensorflow:Assets written to: ./imbd_model/prst_model_1\\assets\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.4275 - auc_1: 0.8834 - precision_1: 0.7869 - recall_1: 0.8054 - val_loss: 0.3347 - val_auc_1: 0.9352 - val_precision_1: 0.8820 - val_recall_1: 0.8158\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2286 - auc_1: 0.9673 - precision_1: 0.9091 - recall_1: 0.9098INFO:tensorflow:Assets written to: ./imbd_model/prst_model_2\\assets\n",
      "391/391 [==============================] - 46s 119ms/step - loss: 0.2286 - auc_1: 0.9673 - precision_1: 0.9091 - recall_1: 0.9098 - val_loss: 0.3670 - val_auc_1: 0.9346 - val_precision_1: 0.8028 - val_recall_1: 0.9195\n"
     ]
    }
   ],
   "source": [
    "adam_opt = Adam(0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# save the model after each epoch\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./imbd_model/prst_model_{epoch}',\n",
    "        save_freq='epoch',\n",
    "        ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='./imbd_logs',\n",
    "        profile_batch=0, \n",
    "        )\n",
    "]\n",
    "\n",
    "# define relevant metrics\n",
    "metrics = [\n",
    "    tf.keras.metrics.AUC(),\n",
    "    tf.keras.metrics.Precision(),\n",
    "    tf.keras.metrics.Recall()\n",
    "]\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    optimizer=adam_opt,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=metrics,\n",
    "#     run_eagerly=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train, validation_data=test, callbacks=callbacks, epochs=2, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
